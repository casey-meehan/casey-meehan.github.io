<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Casey Meehan</title>
  <meta property="og:title" content="Casey Meehan" />
  <meta property="og:locale" content="en_US" />
  <meta name="description" content="Casey's website!" />
  <meta property="og:description" content="Casey's website" />
  <meta property="og:site_name" content="Casey Meehan" />
  <link rel="stylesheet" href="caseytemplate.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
</head>
<body>
  <center>

    <nav class="navbar navbar-expand-lg navbar-light bg-light">
        <a class="navbar-brand" href="#">Casey Meehan</a>
        <div class="collapse navbar-collapse" id="navbarText">
          <ul class="navbar-nav">
            <li class="nav-item">
              <a class="nav-link" href="#projects">Projects</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#contact">Contact</a>
            </li>
          </ul>
        </div>
    </nav>

  <br>
    <img src="casey_restaurant.jpg"/>
    <br>
    <div id="about">
      <p>
       <b>Graduating in 2023!</b> I'm a fifth year PhD student at UCSD advised by <a href="https://cseweb.ucsd.edu/~kamalika/">Kamalika Chaudhuri</a>. My research focuses on personal data privacy in machine learning from two angles: 1) understanding and quantifying how large models memorize their training data, which can lead to leaking individuals' sensitive information, and 2) taking an application-specific approach to offering provable privacy in different ML domains.  
     </p>

    </div>
    <div id="projects">

      <h2>Projects</h2>
      <ul>
	<li>Casey Meehan, Florian Bordes, Pascal Vincent, Kamalika Chaudhuri, Chuan Guo"<a class="paper-link" href="https://arxiv.org/abs/2304.13850">Do SSL Models Have Deja Vu? A Case of Unintended Memorization in Self-Supervised Learning</a>" Preprint, 2023 <br> 
	SSL models have been shown to learn remarkably useful representations of images without access to any labels. However, it was unknown whether SSL models memorize their training images and if so, how. To answer this open question, we identified a new variety of data memorization, Déjà Vu. Given a small generic crop of an SSL training image (e.g. a patch of rippling water), we are able to generatively reconstruct the remainder of the image (e.g. the black swan floating next to that very patch of water) using a novel diffusion-based extraction method. To quantify the degree of Déjà Vu occurring in SSL models, we propose a variety of numerical tests that distinguish unwanted memorization behavior from expected correlation behavior. Given the jarring privacy implications of this finding, we propose potential mitigation strategies. <a href="https://www.linkedin.com/posts/metaai_do-ssl-models-have-d%C3%A9j%C3%A0-vu-a-case-of-unintended-activity-7059205121544515586-OFiO?utm_source=share&utm_medium=member_desktop">LinkedIn Post</a> <br> <br> </li>	 
        <li>Casey Meehan, Khalil Mrini, Kamalika Chaudhuri"<a class="paper-link" href="https://arxiv.org/abs/2205.04605">Sentence-level Privacy for Document Embeddings</a>" ACL, 2022 <br> 
	User language data can contain highly sensitive personal content. As such, it is imperative to offer users a strong and interpretable privacy guarantee when learning from their data. In this work, we propose SentDP: pure local differential privacy at the sentence level for a single user document. We propose a novel technique, DeepCandidate, that combines concepts from robust statistics and language modeling to produce high-dimensional, general-purpose $\epsilon$-SentDP document embeddings. This guarantees that any single sentence in a document can be substituted with any other sentence while keeping the embedding $\epsilon$-indistinguishable.  <br> <br> </li>	 
        <li>Casey Meehan, Amrita Roy Chowdhury, Kamalika Chaudhuri, Somesh Jha. "<a class="paper-link" href="https://arxiv.org/abs/2106.06603">Privacy Implications of Shuffling</a>" ICLR, 2022 <br> 
	Here we formalize how non-uniform random shuffling of users' private data can provide a strong notion of inferential privacy (preventing inference about your data using others' data) while still preserving broad trends within the aggregate. For example, shuffling can block attacks that leverage your family's medical data to make inferences on your medical data. <br> <br> </li>	 
	<li>Tatsuki Koga, Casey Meehan, Kamalika Chaudhuri. "<a class="paper-link" href="https://arxiv.org/abs/2201.04762">Privacy Amplification by Subsampling in Time Domain</a>" AISTATS, 2022 <br> 
	Here we show how subsampling in the time domain -- a signal processing primitive -- provides privacy ampification, reducing the privacy cost while still availing underlying time-series trends. Using a novel analysis, we show the significant reduction in sensitivity provided by time-domain subsampling and propose a corresponding new class of privacy mechanisms. <br> <br> </li>	 
        <li>Casey Meehan, Kamalika Chaudhuri. "<a class = "paper-link" href="https://arxiv.org/abs/2102.11955">Location Trace Privacy Under Conditional Priors</a>" AISTATS, 2021 <br>
 	In this project we analyze how to provide meaningful local privacy to sequences of individuals' locations when they are captured close together in time (traces). See corresponding blogpost <a href="https://ucsdml.github.io/jekyll/update/2021/05/10/location-trace-privacy.html">here</a>. <br> <br></li>
        <li>Casey Meehan, Kamalika Chaudhuri, Sanjoy Dasgupta. "<a class = "paper-link" href="https://arxiv.org/abs/2004.05675">A Non-Parametric Test to Detect Data-Copying in Generative Models</a>" AISTATS, 2020 <br> 
	It is not clear how to determine whether a generative model is overfitting its dataset. This problem is exasperated by the fact that contemporary generative models have intractable likelihoods. In this project, we propose a new notion of overfitting, data-copying, wherein a generative model produces examples that are closer to its training set than a held out test set woudl be. See the companion blogpost <a href="https://ucsdml.github.io/jekyll/update/2020/08/03/how-to-detect-data-copying-in-generative-models.html">here</a>.<br> <br></li>
      </ul>

    </div>
    <div id="contact">
      <h2>Contact</h2>
      Email: cmeehan at eng.ucsd.edu <br>
      Twitter: <a href="https://twitter.com/kc_meehan">@kc_meehan</a> <br>
      <a href="cv.pdf">CV</a>      
    </div>
    <div id="scroll-padding">
    </div>
  </center>

</body>

</html>
